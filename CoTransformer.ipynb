{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import ViTModel, BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTransformer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CoTransformer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads=8)\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        co_transformed_repr = self.layer_norm(self.linear(attn_output) + query)\n",
    "        return co_transformed_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes, dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.co_transformer_vit_to_bert = CoTransformer(dim=dim)\n",
    "        self.co_transformer_bert_to_vit = CoTransformer(dim=dim)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_features=dim*2, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=256, out_features=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, img_query, img_key, img_value, ques_query, ques_key, ques_value):\n",
    "        joint_repr_vit_to_bert = self.co_transformer_vit_to_bert(ques_query, img_key, img_value)\n",
    "        joint_repr_bert_to_vit = self.co_transformer_bert_to_vit(img_query, ques_key, ques_value)\n",
    "        combined_repr = torch.cat((joint_repr_vit_to_bert, joint_repr_bert_to_vit), dim=-1)\n",
    "        x = self.layer1(combined_repr)\n",
    "        x = self.layer2(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, question, answer = self.data[index]\n",
    "        return image, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageName(image_path, image_id):\n",
    "\n",
    "    path = image_path+\"COCO_train2014_\"\n",
    "    output = \"0\" * (12 - len(str(image_id))) + str(image_id)\n",
    "    path = path+output+\".jpg\"\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterMajoritySingleWord(answer_list):\n",
    "\n",
    "    single_word_answers = [entry[\"answer\"] for entry in answer_list if len(entry[\"answer\"].split()) == 1]\n",
    "    if (len(single_word_answers) == 0):\n",
    "        single_word_answers = [entry[\"answer\"] for entry in answer_list]\n",
    "\n",
    "    answer_counts = Counter(single_word_answers)\n",
    "\n",
    "    majority_answer_count = max(answer_counts.values())\n",
    "    majority_answers = [answer for answer, count in answer_counts.items() if count == majority_answer_count]\n",
    "\n",
    "    return majority_answers[0]\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    return cv2.resize(image, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"./Subset_train2014\"\n",
    "\n",
    "# with open('output_image_questions.json') as questions_json:\n",
    "#     images_questions = json.load(questions_json)\n",
    "\n",
    "# with open('output_questions_answers.json') as questions_json:\n",
    "#     questions_answers = json.load(questions_json)\n",
    "\n",
    "# # print(list(images_questions.keys())[:5])\n",
    "# # print(list(questions_answers.values())[:5])\n",
    "\n",
    "# # print(filterMajoritySingleWord(questions_answers['36000']))\n",
    "\n",
    "# l = [(image_id, question['question'], str(question['question_id'])) for image_id, questions in images_questions.items() for question in questions]\n",
    "\n",
    "# dummy_data = [(tup[0], tup[1], filterMajoritySingleWord(questions_answers[tup[2]])) for tup in l]\n",
    "# dummy_data = [data for data in dummy_data if data[2] != \"\"]\n",
    "\n",
    "# df = pd.DataFrame(dummy_data, columns=['image', 'question_id', 'answer'])\n",
    "# df.to_csv(\"Final.csv\", index=False)\n",
    "\n",
    "# del l\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(36, 'Is she wearing a bathing suit?', 'yes'), (36, 'What color is the umbrella?', 'pink'), (36, 'Why is the girl holding an umbrella?', 'sun'), (64, 'Who made the cock?', 'rolex'), (64, 'Are there numbers on the clock face?', 'no')]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Final.csv')\n",
    "image_path = \"./Subset_train2014/\"\n",
    "\n",
    "dummy_data = [tuple(row) for row in df.values]\n",
    "print(dummy_data[:5])\n",
    "\n",
    "answer_vocabulary = set([answer for _, _, answer in dummy_data])\n",
    "# print(answer_vocabulary)\n",
    "\n",
    "# # Create a mapping between answers and class label numbers\n",
    "answer_to_label = {answer: label for label, answer in enumerate(answer_vocabulary)}\n",
    "\n",
    "dummy_data_new = [(load_image(getImageName(image_path, image_id)), question, answer_to_label.get(answer, -1)) for (image_id, question, answer) in dummy_data]\n",
    "\n",
    "# for i, (image_id, question, answer) in enumerate(dummy_data):\n",
    "#     label_number = answer_to_label.get(answer, -1)  # Use -1 as default label for unknown answers\n",
    "#     image = load_image(getImageName(image_path, image_id))\n",
    "#     dummy_data[i] = (image, question, label_number)\n",
    "\n",
    "dummy_data = dummy_data_new\n",
    "\n",
    "print(dummy_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dummy_data, test_size=0.2)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(MyDataset(train_data), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(MyDataset(val_data), batch_size=32, shuffle=False)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(answer_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(num_classes=num_classes, dim=768)  # Assuming dim=768 for ViT and BERT\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, questions, answers in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Process images\n",
    "        img_output = vit_model(pixel_values=images)\n",
    "        img_query, img_key, img_value = img_output.last_hidden_state.split(1, dim=-1)\n",
    "\n",
    "        # Tokenize questions\n",
    "        question_tokens = tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
    "        ques_output = bert_model(**question_tokens)\n",
    "        ques_query, ques_key, ques_value = ques_output.last_hidden_state.split(1, dim=-1)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(img_query.squeeze(0), img_key.squeeze(0), img_value.squeeze(0), ques_query.squeeze(0), ques_key.squeeze(0), ques_value.squeeze(0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, answers)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, questions, answers in val_loader:\n",
    "            # Process images\n",
    "            img_output = vit_model(pixel_values=images)\n",
    "            img_query, img_key, img_value = img_output.last_hidden_state.split(1, dim=-1)\n",
    "\n",
    "            # Tokenize questions\n",
    "            question_tokens = tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
    "            ques_output = bert_model(**question_tokens)\n",
    "            ques_query, ques_key, ques_value = ques_output.last_hidden_state.split(1, dim=-1)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(img_query.squeeze(0), img_key.squeeze(0), img_value.squeeze(0), ques_query.squeeze(0), ques_key.squeeze(0), ques_value.squeeze(0))\n",
    "\n",
    "            # Compute loss\n",
    "            val_loss += criterion(output, answers).item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += answers.size(0)\n",
    "            correct += (predicted == answers).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {(correct/total)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
