{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-30T06:32:59.015021Z","iopub.execute_input":"2024-04-30T06:32:59.015369Z","iopub.status.idle":"2024-04-30T06:33:00.146758Z","shell.execute_reply.started":"2024-04-30T06:32:59.015345Z","shell.execute_reply":"2024-04-30T06:33:00.145095Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: JOBLIB_TEMP_FOLDER=/tmp\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nimport zipfile\nimport os\nfrom tqdm import tqdm\n%env JOBLIB_TEMP_FOLDER=/tmp\n# Define the URLs for the VQA v2 dataset\n_URLS = {\n    \"questions\": {\n        \"train\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\",\n        \"val\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\",\n        \"test-dev\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip\",\n        \"test\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip\",\n    },\n    \"annotations\": {\n        \"train\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\",\n        \"val\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\",\n    },\n    \"images\": {\n        \"train\": \"http://images.cocodataset.org/zips/train2014.zip\",\n        \"val\": \"http://images.cocodataset.org/zips/val2014.zip\",\n        \"test-dev\": \"http://images.cocodataset.org/zips/test2015.zip\",\n        \"test\": \"http://images.cocodataset.org/zips/test2015.zip\",\n    },\n}\n\n# Create a folder to store the dataset files\noutput_dir = \"/kaggle/temp/vqa_v2_data\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Function to download and extract zip files\ndef download_and_extract(url, output_path):\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 KB\n    t = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n    \n    with open(output_path, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            t.update(len(data))\n            file.write(data)\n    t.close()\n\n    with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n        zip_ref.extractall(output_dir)\n\n# Download and extract datasets\nfor key, value in _URLS.items():\n    for name, url in value.items():\n        zip_file_path = os.path.join(output_dir, f\"{key}_{name}.zip\")\n        print(f\"Downloading {name} {key} dataset from {url}...\")\n        download_and_extract(url, zip_file_path)\n        print(f\"Finished downloading and extracting {name} {key} dataset.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T07:02:52.025322Z","iopub.execute_input":"2024-04-30T07:02:52.025743Z","iopub.status.idle":"2024-04-30T07:33:28.771882Z","shell.execute_reply.started":"2024-04-30T07:02:52.025712Z","shell.execute_reply":"2024-04-30T07:33:28.770666Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"env: JOBLIB_TEMP_FOLDER=/tmp\nDownloading train questions dataset from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7.24M/7.24M [00:00<00:00, 9.95MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting train questions dataset.\nDownloading val questions dataset from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3.49M/3.49M [00:00<00:00, 5.24MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting val questions dataset.\nDownloading test-dev questions dataset from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8.97M/8.97M [00:00<00:00, 10.4MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting test-dev questions dataset.\nDownloading test questions dataset from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8.97M/8.97M [00:00<00:00, 10.8MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting test questions dataset.\nDownloading train annotations dataset from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 21.7M/21.7M [00:01<00:00, 17.4MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting train annotations dataset.\nDownloading val annotations dataset from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10.5M/10.5M [00:00<00:00, 11.3MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting val annotations dataset.\nDownloading train images dataset from http://images.cocodataset.org/zips/train2014.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.5G/13.5G [05:33<00:00, 40.5MiB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting train images dataset.\nDownloading val images dataset from http://images.cocodataset.org/zips/val2014.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6.65G/6.65G [02:57<00:00, 37.4MiB/s] \n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting val images dataset.\nDownloading test-dev images dataset from http://images.cocodataset.org/zips/test2015.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.3G/13.3G [09:16<00:00, 23.8MiB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting test-dev images dataset.\nDownloading test images dataset from http://images.cocodataset.org/zips/test2015.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.3G/13.3G [05:36<00:00, 39.4MiB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Finished downloading and extracting test images dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"def download_and_extract(url, output_path):\n    # Downloading the zip file\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 KB\n    t = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n    \n    with open(output_path, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            t.update(len(data))\n            file.write(data)\n    t.close()\n\n    # Extracting the zip file\n    with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n        zip_ref.extractall(output_dir)\n\n# Download and extract all datasets\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T06:54:16.884728Z","iopub.status.idle":"2024-04-30T06:54:16.885084Z","shell.execute_reply.started":"2024-04-30T06:54:16.884893Z","shell.execute_reply":"2024-04-30T06:54:16.884906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, value in _URLS.items():\n    for name, url in value.items():\n        zip_file_path = os.path.join(output_dir, f\"{key}_{name}.zip\")\n        print(f\"Downloading {name} {key} dataset from {url}...\")\n        download_and_extract(url, zip_file_path)\n        print(f\"Finished downloading and extracting {name} {key} dataset.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import shutil\nshutil.make_archive(archn, 'zip', out_folder)","metadata":{}},{"cell_type":"code","source":"import shutil\narchn = \"datasets\"\nshutil.make_archive(archn, 'zip', output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T07:41:50.814760Z","iopub.execute_input":"2024-04-30T07:41:50.815186Z","iopub.status.idle":"2024-04-30T07:59:20.699615Z","shell.execute_reply.started":"2024-04-30T07:41:50.815141Z","shell.execute_reply":"2024-04-30T07:59:20.697953Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1776\u001b[0m, in \u001b[0;36mZipFile.write\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(zinfo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:198\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[43mfdst_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1141\u001b[0m, in \u001b[0;36m_ZipWriteFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nbytes\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      2\u001b[0m archn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:1124\u001b[0m, in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1121\u001b[0m             os\u001b[38;5;241m.\u001b[39mchdir(root_dir)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_cwd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:1009\u001b[0m, in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path):\n\u001b[1;32m   1008\u001b[0m     arcname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(arcdirpath, name)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1775\u001b[0m, in \u001b[0;36mZipFile.write\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(zinfo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[1;32m   1776\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(src, dest, \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1153\u001b[0m, in \u001b[0;36m_ZipWriteFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compressor\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[0;32m-> 1153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zinfo\u001b[38;5;241m.\u001b[39mcompress_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"],"ename":"OSError","evalue":"[Errno 28] No space left on device","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}